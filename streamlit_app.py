import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import base64
import re
import requests
import jieba  # ç”¨äºä¸­æ–‡åˆ†è¯ï¼Œæé«˜åŒ¹é…ç²¾åº¦

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stApp { max-width: 1200px; margin: 0 auto; }
    .stFileUploader { width: 100%; }
    .highlight-conflict { background-color: #ffeeba; padding: 2px 4px; border-radius: 3px; }
    .clause-box { border-left: 4px solid #007bff; padding: 10px; margin: 10px 0; background-color: #f8f9fa; }
    .compliance-ok { border-left: 4px solid #28a745; }
    .compliance-warning { border-left: 4px solid #ffc107; }
    .compliance-conflict { border-left: 4px solid #dc3545; }
    .model-response { background-color: #f0f2f6; padding: 15px; border-radius: 5px; margin: 10px 0; }
</style>
""", unsafe_allow_html=True)

# é…ç½®Qwen APIå‚æ•° - ä½¿ç”¨æŒ‡å®šçš„APIé“¾æ¥
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

def call_qwen_api(prompt, api_key):
    """è°ƒç”¨Qwenå¤§æ¨¡å‹APIï¼Œä½¿ç”¨æŒ‡å®šçš„APIé“¾æ¥"""
    if not api_key:
        st.error("Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨å·¦ä¾§æ è¾“å…¥å¯†é’¥")
        return None
        
    try:
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        # æ„å»ºç¬¦åˆAPIè¦æ±‚çš„è¯·æ±‚æ•°æ®
        data = {
            "model": "qwen-plus",  # å¯æ ¹æ®éœ€è¦æ›´æ¢ä¸ºå…¶ä»–Qwenæ¨¡å‹å¦‚qwen-max
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            "max_tokens": 5000
        }
        
        # ä½¿ç”¨æŒ‡å®šçš„APIé“¾æ¥å‘é€POSTè¯·æ±‚
        response = requests.post(
            QWEN_API_URL,
            headers=headers,
            json=data,
            timeout=60
        )
        
        # æ£€æŸ¥HTTPå“åº”çŠ¶æ€
        if response.status_code != 200:
            st.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}ï¼Œå“åº”: {response.text}")
            return None
            
        # è§£æJSONå“åº”
        response_json = response.json()
        
        # æ£€æŸ¥å“åº”ç»“æ„
        if "choices" not in response_json or len(response_json["choices"]) == 0:
            st.error("APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
            return None
            
        return response_json["choices"][0]["message"]["content"]
        
    except requests.exceptions.Timeout:
        st.error("APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·é‡è¯•")
        return None
    except Exception as e:
        st.error(f"è°ƒç”¨Qwen APIå¤±è´¥: {str(e)}")
        return None

def extract_text_from_pdf(file):
    """ä»PDFæå–æ–‡æœ¬ï¼Œä¼˜åŒ–ä¸­æ–‡å¤„ç†"""
    try:
        pdf_reader = PdfReader(file)
        text = ""
        for page in pdf_reader.pages:
            page_text = page.extract_text() or ""
            # å¤„ç†ä¸­æ–‡ç©ºæ ¼å’Œæ¢è¡Œé—®é¢˜
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
        return text
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

def split_into_clauses(text):
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ¬¾ï¼Œå¢å¼ºä¸­æ–‡æ¡æ¬¾è¯†åˆ«ï¼Œå¹¶é™åˆ¶æœ€å¤§æ¡æ¬¾æ•°é‡"""
    # å¢å¼ºä¸­æ–‡æ¡æ¬¾æ¨¡å¼è¯†åˆ«
    patterns = [
        # ä¸­æ–‡æ¡æ¬¾å¸¸è§æ ¼å¼
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+|$)',  # ç¬¬ä¸€æ¡ã€ç¬¬äºŒæ¡æ ¼å¼
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+|$)',  # ä¸€ã€äºŒã€ä¸‰ã€æ ¼å¼
        r'(\d+\.\s+.*?)(?=\d+\.\s+|$)',  # 1. 2. 3. æ ¼å¼
        r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+.*?)(?=\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+|$)',  # (ä¸€) (äºŒ) æ ¼å¼
        r'(\([1-9]+\)\s+.*?)(?=\([1-9]+\)\s+|$)',  # (1) (2) æ ¼å¼
        r'(ã€[^\ã€‘]+ã€‘\s+.*?)(?=ã€[^\ã€‘]+ã€‘\s+|$)'  # ã€æ ‡é¢˜ã€‘æ ¼å¼
    ]
    
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        if len(clauses) > 3:  # ç¡®ä¿æ‰¾åˆ°è¶³å¤Ÿå¤šçš„æ¡æ¬¾
            # é™åˆ¶æœ€å¤§æ¡æ¬¾æ•°é‡ï¼Œé¿å…UIæ¸²æŸ“é—®é¢˜
            limited_clauses = [clause.strip() for clause in clauses if clause.strip()][:80]
            return limited_clauses
    
    # æŒ‰ä¸­æ–‡æ ‡ç‚¹åˆ†å‰²æ®µè½
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p) > 10]  # è¿‡æ»¤è¿‡çŸ­å†…å®¹
    # é™åˆ¶æœ€å¤§æ¡æ¬¾æ•°é‡
    return paragraphs[:80]

def chinese_text_similarity(text1, text2):
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œä½¿ç”¨åˆ†è¯ååŒ¹é…"""
    # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
    words1 = list(jieba.cut(text1))
    words2 = list(jieba.cut(text2))
    
    # è®¡ç®—åˆ†è¯åçš„ç›¸ä¼¼åº¦
    return SequenceMatcher(None, words1, words2).ratio()

def match_clauses(clauses1, clauses2):
    """åŒ¹é…ä¸¤ä¸ªæ–‡æ¡£ä¸­çš„ç›¸ä¼¼æ¡æ¬¾ï¼Œä¼˜åŒ–ä¸­æ–‡åŒ¹é…"""
    matched_pairs = []
    used_indices = set()
    
    # é™åˆ¶åŒ¹é…å¯¹æ•°é‡ï¼Œé¿å…è¿‡å¤šUIå…ƒç´ 
    max_matches = min(50, len(clauses1), len(clauses2))
    
    for i, clause1 in enumerate(clauses1[:max_matches]):
        best_match = None
        best_ratio = 0.25  # é™ä½ä¸­æ–‡åŒ¹é…é˜ˆå€¼
        best_j = -1
        
        for j, clause2 in enumerate(clauses2):
            if j not in used_indices:
                # ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
                ratio = chinese_text_similarity(clause1, clause2)
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_match = clause2
                    best_j = j
        
        if best_match:
            matched_pairs.append((clause1, best_match, best_ratio))
            used_indices.add(best_j)
    
    unmatched1 = [clause for i, clause in enumerate(clauses1) 
                 if i not in [idx for idx, _ in enumerate(matched_pairs)]]
    unmatched2 = [clause for j, clause in enumerate(clauses2) if j not in used_indices]
    
    return matched_pairs, unmatched1, unmatched2

def create_download_link(content, filename, text):
    """ç”Ÿæˆä¸‹è½½é“¾æ¥"""
    b64 = base64.b64encode(content.encode()).decode()
    return f'<a href="data:file/txt;base64,{b64}" download="{filename}">{text}</a>'

def analyze_compliance_with_qwen(clause1, clause2, filename1, filename2, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†ææ¡æ¬¾åˆè§„æ€§ï¼Œä¼˜åŒ–ä¸­æ–‡æç¤ºè¯"""
    # ä¼˜åŒ–ä¸­æ–‡æç¤ºè¯ï¼Œæ›´ç¬¦åˆä¸­æ–‡æ¡æ¬¾åˆ†æåœºæ™¯
    prompt = f"""
    è¯·ä»”ç»†åˆ†æä»¥ä¸‹ä¸¤ä¸ªä¸­æ–‡æ¡æ¬¾çš„åˆè§„æ€§ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦å­˜åœ¨å†²çªï¼š
    
    {filename1} æ¡æ¬¾ï¼š{clause1}
    
    {filename2} æ¡æ¬¾ï¼š{clause2}
    
    è¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”¨ä¸­æ–‡è¿›è¡Œè¯¦ç»†åˆ†æï¼š
    1. ç›¸ä¼¼åº¦è¯„ä¼°ï¼šè¯„ä¼°ä¸¤ä¸ªæ¡æ¬¾çš„ç›¸ä¼¼ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰
    2. å·®å¼‚ç‚¹åˆ†æï¼šè¯¦ç»†æŒ‡å‡ºä¸¤ä¸ªæ¡æ¬¾åœ¨è¡¨è¿°ã€èŒƒå›´ã€è¦æ±‚ç­‰æ–¹é¢çš„ä¸»è¦å·®å¼‚
    3. åˆè§„æ€§åˆ¤æ–­ï¼šåˆ¤æ–­æ˜¯å¦å­˜åœ¨å†²çªï¼ˆæ— å†²çª/è½»å¾®å†²çª/ä¸¥é‡å†²çªï¼‰
    4. å†²çªåŸå› ï¼šå¦‚æœå­˜åœ¨å†²çªï¼Œè¯·å…·ä½“è¯´æ˜å†²çªçš„åŸå› å’Œå¯èƒ½å¸¦æ¥çš„å½±å“
    5. å»ºè®®ï¼šé’ˆå¯¹å‘ç°çš„é—®é¢˜ï¼Œç»™å‡ºä¸“ä¸šçš„å¤„ç†å»ºè®®
    
    åˆ†ææ—¶è¯·ç‰¹åˆ«æ³¨æ„ä¸­æ–‡æ³•å¾‹/åˆåŒæ¡æ¬¾ä¸­å¸¸ç”¨è¡¨è¿°çš„ç»†å¾®å·®åˆ«ï¼Œ
    å¦‚"åº”å½“"ä¸"å¿…é¡»"ã€"ä¸å¾—"ä¸"ç¦æ­¢"ã€"å¯ä»¥"ä¸"æœ‰æƒ"ç­‰è¯è¯­çš„åŒºåˆ«ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def analyze_standalone_clause_with_qwen(clause, doc_name, api_key):
    """ä½¿ç”¨Qwenå¤§æ¨¡å‹åˆ†æç‹¬ç«‹æ¡æ¬¾ï¼ˆæœªåŒ¹é…çš„æ¡æ¬¾ï¼‰"""
    prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ä¸­æ–‡æ¡æ¬¾çš„å†…å®¹ï¼š
    
    {doc_name} ä¸­çš„æ¡æ¬¾ï¼š{clause}
    
    è¯·ç”¨ä¸­æ–‡è¯„ä¼°è¯¥æ¡æ¬¾çš„ä¸»è¦å†…å®¹ã€æ ¸å¿ƒè¦æ±‚ã€æ½œåœ¨å½±å“å’Œå¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼Œ
    å¹¶ç»™å‡ºç®€è¦åˆ†æå’Œå»ºè®®ã€‚åˆ†ææ—¶è¯·æ³¨æ„ä¸­æ–‡è¡¨è¿°çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§ã€‚
    """
    
    return call_qwen_api(prompt, api_key)

def show_compliance_analysis(text1, text2, filename1, filename2, api_key):
    """æ˜¾ç¤ºåˆè§„æ€§åˆ†æç»“æœï¼Œæ·»åŠ åˆ†é¡µå¤„ç†"""
    # åˆ†å‰²æ¡æ¬¾
    with st.spinner("æ­£åœ¨åˆ†æä¸­æ–‡æ¡æ¬¾ç»“æ„..."):
        clauses1 = split_into_clauses(text1)
        clauses2 = split_into_clauses(text2)
        
        st.success(f"æ¡æ¬¾åˆ†æå®Œæˆ: {filename1} è¯†åˆ«å‡º {len(clauses1)} æ¡æ¡æ¬¾ï¼Œ{filename2} è¯†åˆ«å‡º {len(clauses2)} æ¡æ¡æ¬¾")
    
    # åŒ¹é…æ¡æ¬¾
    with st.spinner("æ­£åœ¨åŒ¹é…ç›¸ä¼¼æ¡æ¬¾..."):
        matched_pairs, unmatched1, unmatched2 = match_clauses(clauses1, clauses2)
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    st.divider()
    col1, col2, col3 = st.columns(3)
    col1.metric(f"{filename1} æ¡æ¬¾æ•°", len(clauses1))
    col2.metric(f"{filename2} æ¡æ¬¾æ•°", len(clauses2))
    col3.metric("åŒ¹é…æ¡æ¬¾æ•°", len(matched_pairs))
    
    # æ˜¾ç¤ºæ¡æ¬¾å¯¹æ¯”å’Œåˆè§„æ€§åˆ†æ
    st.divider()
    st.subheader("ğŸ“Š æ¡æ¬¾åˆè§„æ€§è¯¦ç»†åˆ†æï¼ˆQwenå¤§æ¨¡å‹ï¼‰")
    
    # æ·»åŠ åˆ†é¡µæ§åˆ¶ï¼Œé¿å…ä¸€æ¬¡æ€§æ¸²æŸ“è¿‡å¤šå…ƒç´ 
    total_matches = len(matched_pairs)
    items_per_page = 10  # æ¯é¡µæ˜¾ç¤º10å¯¹æ¡æ¬¾
    total_pages = max(1, (total_matches + items_per_page - 1) // items_per_page)
    
    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ä¸­çš„é¡µç 
    if 'current_page' not in st.session_state:
        st.session_state.current_page = 1
    
    # é¡µç é€‰æ‹©å™¨
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        st.session_state.current_page = st.slider(
            f"æŸ¥çœ‹ç¬¬ {st.session_state.current_page} é¡µï¼Œå…± {total_pages} é¡µ",
            1, total_pages, st.session_state.current_page
        )
    
    # è®¡ç®—å½“å‰é¡µæ˜¾ç¤ºçš„æ¡æ¬¾èŒƒå›´
    start_idx = (st.session_state.current_page - 1) * items_per_page
    end_idx = min(start_idx + items_per_page, total_matches)
    current_pairs = matched_pairs[start_idx:end_idx]
    
    # åˆ†æå½“å‰é¡µçš„åŒ¹é…å¯¹
    for i, (clause1, clause2, ratio) in enumerate(current_pairs, start=start_idx + 1):
        st.markdown(f"### åŒ¹é…æ¡æ¬¾å¯¹ {i}ï¼ˆç›¸ä¼¼åº¦: {ratio:.2%}ï¼‰")
        
        # ä½¿ç”¨expanderæŠ˜å æ¡æ¬¾å†…å®¹ï¼Œå‡å°‘UIå…ƒç´ æ•°é‡
        with st.expander(f"{filename1} æ¡æ¬¾å†…å®¹", expanded=False):
            st.markdown(f'<div class="clause-box">{clause1}</div>', unsafe_allow_html=True)
        
        with st.expander(f"{filename2} æ¡æ¬¾å†…å®¹", expanded=False):
            st.markdown(f'<div class="clause-box">{clause2}</div>', unsafe_allow_html=True)
        
        with st.spinner(f"æ­£åœ¨åˆ†ææ¡æ¬¾å¯¹ {i}..."):
            analysis = analyze_compliance_with_qwen(clause1, clause2, filename1, filename2, api_key)
        
        if analysis:
            st.markdown('<div class="model-response"><strong>Qwenå¤§æ¨¡å‹åˆ†æç»“æœ:</strong><br>' + analysis + '</div>', unsafe_allow_html=True)
        
        st.divider()
    
    # æœªåŒ¹é…çš„æ¡æ¬¾åˆ†æ - ä½¿ç”¨åˆ†é¡µ
    st.subheader("æœªåŒ¹é…æ¡æ¬¾åˆ†æ")
    
    # å¤„ç†ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„æœªåŒ¹é…æ¡æ¬¾
    st.markdown(f"#### {filename1} ä¸­ç‹¬æœ‰çš„æ¡æ¬¾ ({len(unmatched1)})")
    if len(unmatched1) > 0:
        # åˆ†é¡µå¤„ç†æœªåŒ¹é…æ¡æ¬¾
        unmatched1_per_page = 5
        unmatched1_pages = max(1, (len(unmatched1) + unmatched1_per_page - 1) // unmatched1_per_page)
        
        if 'unmatched1_page' not in st.session_state:
            st.session_state.unmatched1_page = 1
            
        st.session_state.unmatched1_page = st.slider(
            f"{filename1} æœªåŒ¹é…æ¡æ¬¾é¡µç ",
            1, unmatched1_pages, st.session_state.unmatched1_page
        )
        
        start = (st.session_state.unmatched1_page - 1) * unmatched1_per_page
        end = min(start + unmatched1_per_page, len(unmatched1))
        
        for i, clause in enumerate(unmatched1[start:end], start=start + 1):
            with st.expander(f"æ¡æ¬¾ {i}", expanded=False):
                st.markdown(f'<div class="clause-box">{clause}</div>', unsafe_allow_html=True)
                with st.spinner(f"æ­£åœ¨åˆ†ææ¡æ¬¾ {i}..."):
                    analysis = analyze_standalone_clause_with_qwen(clause, filename1, api_key)
                if analysis:
                    st.markdown('<div class="model-response">' + analysis + '</div>', unsafe_allow_html=True)
    
    # å¤„ç†ç¬¬äºŒä¸ªæ–‡ä»¶çš„æœªåŒ¹é…æ¡æ¬¾
    st.markdown(f"#### {filename2} ä¸­ç‹¬æœ‰çš„æ¡æ¬¾ ({len(unmatched2)})")
    if len(unmatched2) > 0:
        # åˆ†é¡µå¤„ç†æœªåŒ¹é…æ¡æ¬¾
        unmatched2_per_page = 5
        unmatched2_pages = max(1, (len(unmatched2) + unmatched2_per_page - 1) // unmatched2_per_page)
        
        if 'unmatched2_page' not in st.session_state:
            st.session_state.unmatched2_page = 1
            
        st.session_state.unmatched2_page = st.slider(
            f"{filename2} æœªåŒ¹é…æ¡æ¬¾é¡µç ",
            1, unmatched2_pages, st.session_state.unmatched2_page
        )
        
        start = (st.session_state.unmatched2_page - 1) * unmatched2_per_page
        end = min(start + unmatched2_per_page, len(unmatched2))
        
        for i, clause in enumerate(unmatched2[start:end], start=start + 1):
            with st.expander(f"æ¡æ¬¾ {i}", expanded=False):
                st.markdown(f'<div class="clause-box">{clause}</div>', unsafe_allow_html=True)
                with st.spinner(f"æ­£åœ¨åˆ†ææ¡æ¬¾ {i}..."):
                    analysis = analyze_standalone_clause_with_qwen(clause, filename2, api_key)
                if analysis:
                    st.markdown('<div class="model-response">' + analysis + '</div>', unsafe_allow_html=True)

# æ·»åŠ ä¸»ç¨‹åºå…¥å£ï¼ˆåŸä»£ç ä¸­ç¼ºå°‘è¿™éƒ¨åˆ†ï¼‰
def main():
    st.title("Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·")
    
    # ä¾§è¾¹æ è®¾ç½®
    with st.sidebar:
        st.header("è®¾ç½®")
        api_key = st.text_input("Qwen API å¯†é’¥", type="password")
        st.markdown("""
        è¯·è¾“å…¥æ‚¨çš„Qwen APIå¯†é’¥ä»¥ä½¿ç”¨æœ¬å·¥å…·ã€‚
        """)
    
    # ä¸Šä¼ æ–‡ä»¶
    st.header("ä¸Šä¼ PDFæ–‡ä»¶")
    col1, col2 = st.columns(2)
    with col1:
        file1 = st.file_uploader("ä¸Šä¼ ç¬¬ä¸€ä¸ªPDFæ–‡ä»¶", type="pdf", key="file1")
    with col2:
        file2 = st.file_uploader("ä¸Šä¼ ç¬¬äºŒä¸ªPDFæ–‡ä»¶", type="pdf", key="file2")
    
    # åˆ†ææŒ‰é’®
    if st.button("å¼€å§‹åˆè§„æ€§åˆ†æ") and file1 and file2 and api_key:
        with st.spinner("æ­£åœ¨æå–PDFæ–‡æœ¬..."):
            text1 = extract_text_from_pdf(file1)
            text2 = extract_text_from_pdf(file2)
        
        if text1 and text2:
            show_compliance_analysis(text1, text2, file1.name, file2.name, api_key)
        else:
            st.error("æ— æ³•ä»ä¸€ä¸ªæˆ–å¤šä¸ªPDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆã€‚")

if __name__ == "__main__":
    main()
