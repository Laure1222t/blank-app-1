import streamlit as st
from PyPDF2 import PdfReader
from difflib import SequenceMatcher
import re
import requests
import jieba
import time
from functools import lru_cache

# åˆå§‹åŒ– jieba åˆ†è¯å™¨ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
jieba.initialize()

# è®¾ç½®é¡µé¢æ ‡é¢˜å’Œå›¾æ ‡
st.set_page_config(
    page_title="Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·",
    page_icon="ğŸ“„",
    layout="wide"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
<style>
    .stApp { max-width: 1200px; margin: 0 auto; }
    .clause-box { border-left: 4px solid #007bff; padding: 10px; margin: 10px 0; background-color: #f8f9fa; }
    .model-response { background-color: #f0f2f6; padding: 15px; border-radius: 5px; margin: 10px 0; }
    .comparison-section { border: 1px solid #e6e6e6; border-radius: 8px; padding: 15px; margin-bottom: 20px; }
    .progress-container { margin: 20px 0; }
</style>
""", unsafe_allow_html=True)

# é…ç½®Qwen APIå‚æ•°
QWEN_API_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"

# æ€§èƒ½ä¼˜åŒ–ï¼šè®¾ç½®APIè°ƒç”¨è¶…æ—¶å’Œé‡è¯•æœºåˆ¶
def call_qwen_api(prompt, api_key, max_retries=2):
    """è°ƒç”¨Qwenå¤§æ¨¡å‹APIï¼Œå¸¦é‡è¯•æœºåˆ¶"""
    if not api_key:
        st.error("Qwen APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨å·¦ä¾§æ è¾“å…¥å¯†é’¥")
        return None
        
    retry_count = 0
    while retry_count <= max_retries:
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
            
            data = {
                "model": "qwen-plus",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 1500  # å‡å°‘å•æ¬¡å“åº”é•¿åº¦ï¼ŒåŠ å¿«è¿”å›é€Ÿåº¦
            }
            
            # ç¼©çŸ­è¶…æ—¶æ—¶é—´
            response = requests.post(
                QWEN_API_URL,
                headers=headers,
                json=data,
                timeout=20  # è¶…æ—¶è®¾ç½®ä¸º20ç§’
            )
            
            if response.status_code == 200:
                response_json = response.json()
                if "choices" in response_json and len(response_json["choices"]) > 0:
                    return response_json["choices"][0]["message"]["content"]
                else:
                    st.error("APIè¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ")
                    return None
            else:
                st.error(f"APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                retry_count += 1
                if retry_count <= max_retries:
                    st.info(f"æ­£åœ¨é‡è¯•...({retry_count}/{max_retries})")
                    time.sleep(1)  # é‡è¯•å‰çŸ­æš‚ç­‰å¾…
                
        except requests.exceptions.Timeout:
            st.error("APIè¯·æ±‚è¶…æ—¶")
            retry_count += 1
            if retry_count <= max_retries:
                st.info(f"æ­£åœ¨é‡è¯•...({retry_count}/{max_retries})")
                time.sleep(1)
        except Exception as e:
            st.error(f"è°ƒç”¨Qwen APIå¤±è´¥: {str(e)}")
            return None
            
    st.error("å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•å®ŒæˆAPIè°ƒç”¨")
    return None

# æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ ç¼“å­˜å’Œè¿›åº¦æç¤º
@st.cache_data(show_spinner=False, ttl=3600)  # ç¼“å­˜1å°æ—¶
def extract_text_from_pdf(file_bytes):
    """ä»PDFæå–æ–‡æœ¬ï¼Œå¸¦è¿›åº¦è·Ÿè¸ª"""
    try:
        pdf_reader = PdfReader(file_bytes)
        text = ""
        total_pages = len(pdf_reader.pages)
        
        # å°æ–‡ä»¶å¿«é€Ÿå¤„ç†
        if total_pages <= 5:
            for page in pdf_reader.pages:
                page_text = page.extract_text() or ""
                page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
                text += page_text
            return text
            
        # å¤§æ–‡ä»¶æ˜¾ç¤ºè¿›åº¦
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, page in enumerate(pdf_reader.pages):
            page_text = page.extract_text() or ""
            page_text = page_text.replace("  ", "").replace("\n", "").replace("\r", "")
            text += page_text
            
            # æ›´æ–°è¿›åº¦
            progress = (i + 1) / total_pages
            progress_bar.progress(progress)
            status_text.text(f"æå–æ–‡æœ¬: å·²å®Œæˆ {i+1}/{total_pages} é¡µ")
            time.sleep(0.01)  # é¿å…UIæ›´æ–°è¿‡äºé¢‘ç¹
            
        # æ¸…ç†è¿›åº¦ç»„ä»¶
        progress_bar.empty()
        status_text.empty()
        return text
        
    except Exception as e:
        st.error(f"æå–æ–‡æœ¬å¤±è´¥: {str(e)}")
        return ""

# æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜æ¡æ¬¾åˆ†å‰²ç»“æœ
@st.cache_data(show_spinner=False, ttl=3600)
def split_into_clauses(text):
    """å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ¡æ¬¾ï¼Œä¼˜åŒ–æ€§èƒ½"""
    # ä¸­æ–‡æ¡æ¬¾å¸¸è§æ ¼å¼
    patterns = [
        r'(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+.*?)(?=ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+æ¡\s+|$)',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+.*?)(?=[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s+|$)',
        r'(\d+\.\s+.*?)(?=\d+\.\s+|$)',
        r'(\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+.*?)(?=\([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\)\s+|$)',
        r'(\([1-9]+\)\s+.*?)(?=\([1-9]+\)\s+|$)',
        r'(ã€[^\ã€‘]+ã€‘\s+.*?)(?=ã€[^\ã€‘]+ã€‘\s+|$)'
    ]
    
    for pattern in patterns:
        clauses = re.findall(pattern, text, re.DOTALL)
        if len(clauses) > 3:
            # è¿‡æ»¤çŸ­æ¡æ¬¾ï¼Œå‡å°‘åç»­è®¡ç®—é‡
            return [clause.strip() for clause in clauses if clause.strip() and len(clause.strip()) > 20]
    
    # æŒ‰æ ‡ç‚¹åˆ†å‰²æ®µè½
    paragraphs = re.split(r'[ã€‚ï¼›ï¼ï¼Ÿ]\s*', text)
    return [p.strip() for p in paragraphs if p.strip() and len(p) > 20]

# æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨lru_cacheç¼“å­˜ç›¸ä¼¼åº¦è®¡ç®—ç»“æœ
@lru_cache(maxsize=10000)
def chinese_text_similarity(text1, text2):
    """è®¡ç®—ä¸­æ–‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œç»“æœç¼“å­˜"""
    words1 = tuple(jieba.cut(text1))  # è½¬æ¢ä¸ºå¯å“ˆå¸Œç±»å‹
    words2 = tuple(jieba.cut(text2))
    return SequenceMatcher(None, words1, words2).ratio()

def match_clauses(clauses1, clauses2):
    """åŒ¹é…æ¡æ¬¾ï¼Œä¼˜åŒ–ç®—æ³•å‡å°‘è®¡ç®—é‡"""
    matched_pairs = []
    used_indices = set()
    len_clauses2 = len(clauses2)
    
    # æ€§èƒ½ä¼˜åŒ–ï¼šé™åˆ¶æœ€å¤§åŒ¹é…æ•°é‡ï¼Œé¿å…è¿‡åº¦è®¡ç®—
    max_matches = min(50, len(clauses1), len_clauses2)
    
    for i, clause1 in enumerate(clauses1):
        if len(matched_pairs) >= max_matches:
            break  # è¾¾åˆ°æœ€å¤§åŒ¹é…æ•°ï¼Œåœæ­¢è®¡ç®—
            
        best_match = None
        best_ratio = 0.3  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘ä½ç›¸ä¼¼åº¦åŒ¹é…
        best_j = -1
        checked = 0
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šé™åˆ¶æ¯ä¸ªæ¡æ¬¾æ£€æŸ¥çš„æ•°é‡
        max_checks = min(15, len_clauses2 - len(used_indices))
        
        for j, clause2 in enumerate(clauses2):
            if j not in used_indices and checked < max_checks:
                ratio = chinese_text_similarity(clause1, clause2)
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_match = clause2
                    best_j = j
                checked += 1
        
        if best_match:
            matched_pairs.append((clause1, best_match, best_ratio))
            used_indices.add(best_j)
    
    return matched_pairs

def analyze_compliance_with_qwen(clause1, clause2, filename1, filename2, api_key):
    """åˆ†ææ¡æ¬¾åˆè§„æ€§ï¼Œç²¾ç®€æç¤ºè¯"""
    # ç²¾ç®€æç¤ºè¯ï¼Œå‡å°‘æ¨¡å‹å¤„ç†æ—¶é—´
    prompt = f"""åˆ†æä»¥ä¸‹ä¸¤ä¸ªæ¡æ¬¾çš„åˆè§„æ€§ï¼š
    {filename1}ï¼š{clause1[:500]}  # é™åˆ¶æ¡æ¬¾é•¿åº¦
    {filename2}ï¼š{clause2[:500]}
    
    è¯·ç®€è¦åˆ†æï¼š1.ç›¸ä¼¼åº¦ 2.å·®å¼‚ç‚¹ 3.æ˜¯å¦å†²çª 4.å»ºè®®
    ç”¨ç®€æ´ä¸­æ–‡å›ç­”ï¼Œæ§åˆ¶åœ¨300å­—ä»¥å†…ã€‚"""
    
    return call_qwen_api(prompt, api_key)

def analyze_single_comparison(base_text, compare_text, base_filename, compare_filename, api_key):
    """å•æ–‡ä»¶å¯¹æ¯”åˆ†æï¼Œæ·»åŠ è¿›åº¦æ§åˆ¶"""
    # æ¡æ¬¾æå–
    with st.spinner(f"æ­£åœ¨åˆ†æ {compare_filename} çš„æ¡æ¬¾ç»“æ„..."):
        base_clauses = split_into_clauses(base_text)
        compare_clauses = split_into_clauses(compare_text)
        
        st.success(f"æ¡æ¬¾åˆ†æå®Œæˆ: {base_filename} è¯†åˆ«å‡º {len(base_clauses)} æ¡ï¼Œ{compare_filename} è¯†åˆ«å‡º {len(compare_clauses)} æ¡")
    
    # æ¡æ¬¾åŒ¹é…
    with st.spinner(f"æ­£åœ¨åŒ¹é…ç›¸ä¼¼æ¡æ¬¾..."):
        matched_pairs = match_clauses(base_clauses, compare_clauses)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    st.divider()
    col1, col2, col3 = st.columns(3)
    col1.metric(f"{base_filename} æ¡æ¬¾æ•°", len(base_clauses))
    col2.metric(f"{compare_filename} æ¡æ¬¾æ•°", len(compare_clauses))
    col3.metric("åŒ¹é…æ¡æ¬¾æ•°", len(matched_pairs))
    
    if not matched_pairs:
        st.info("æœªæ‰¾åˆ°åŒ¹é…çš„æ¡æ¬¾å¯¹")
        return
    
    # åˆ†æ‰¹å¤„ç†åŒ¹é…ç»“æœ
    st.divider()
    st.subheader(f"ğŸ“Š ä¸ {compare_filename} çš„æ¡æ¬¾åˆè§„æ€§åˆ†æ")
    
    batch_size = 3  # æ¯æ‰¹å¤„ç†3å¯¹æ¡æ¬¾
    total_batches = (len(matched_pairs) + batch_size - 1) // batch_size
    
    for batch_idx in range(total_batches):
        start = batch_idx * batch_size
        end = start + batch_size
        batch = matched_pairs[start:end]
        
        st.markdown(f"### åˆ†ææ‰¹æ¬¡ {batch_idx + 1}/{total_batches}")
        progress_bar = st.progress(0)
        
        for i, (clause1, clause2, ratio) in enumerate(batch):
            st.markdown(f"#### åŒ¹é…æ¡æ¬¾å¯¹ {start + i + 1}ï¼ˆç›¸ä¼¼åº¦: {ratio:.2%}ï¼‰")
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.markdown(f'<div class="clause-box"><strong>{base_filename} æ¡æ¬¾:</strong><br>{clause1[:600]}...</div>' 
                           if len(clause1) > 600 else 
                           f'<div class="clause-box"><strong>{base_filename} æ¡æ¬¾:</strong><br>{clause1}</div>', 
                           unsafe_allow_html=True)
            with col_b:
                st.markdown(f'<div class="clause-box"><strong>{compare_filename} æ¡æ¬¾:</strong><br>{clause2[:600]}...</div>'
                           if len(clause2) > 600 else
                           f'<div class="clause-box"><strong>{compare_filename} æ¡æ¬¾:</strong><br>{clause2}</div>',
                           unsafe_allow_html=True)
            
            with st.spinner(f"æ­£åœ¨åˆ†æç¬¬ {start + i + 1} å¯¹æ¡æ¬¾..."):
                analysis = analyze_compliance_with_qwen(clause1, clause2, base_filename, compare_filename, api_key)
            
            if analysis:
                st.markdown(f'<div class="model-response"><strong>åˆ†æç»“æœ:</strong><br>{analysis}</div>', 
                           unsafe_allow_html=True)
            
            # æ›´æ–°æ‰¹æ¬¡è¿›åº¦
            progress = (i + 1) / len(batch)
            progress_bar.progress(progress)
            st.divider()
        
        progress_bar.empty()

# ä¸»ç•Œé¢
st.title("ğŸ“„ Qwen ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…·")
st.markdown("ä¼˜åŒ–ç‰ˆï¼šæ›´å¿«çš„å¤„ç†é€Ÿåº¦ï¼Œæ”¯æŒä¸€å¯¹å¤šæ–‡ä»¶æ¯”å¯¹")

# APIè®¾ç½®
with st.sidebar:
    st.subheader("Qwen API è®¾ç½®")
    qwen_api_key = st.text_input("è¯·è¾“å…¥Qwen APIå¯†é’¥", type="password")
    st.markdown("æç¤ºï¼šAPIå¯†é’¥å¯ä»é˜¿é‡Œäº‘DashScopeæ§åˆ¶å°è·å–")
    
    # æ€§èƒ½è®¾ç½®
    st.subheader("æ€§èƒ½è®¾ç½®")
    max_files = st.slider("æœ€å¤§æ¯”å¯¹æ–‡ä»¶æ•°", 1, 5, 2)
    max_matches_per_file = st.slider("æ¯æ–‡ä»¶æœ€å¤§åŒ¹é…æ•°", 5, 30, 10)

with st.form("upload_form"):
    st.subheader("æ–‡ä»¶ä¸Šä¼ åŒº")
    base_file = st.file_uploader("é€‰æ‹©åŸºå‡†PDFæ–‡ä»¶", type=["pdf"])
    compare_files = st.file_uploader("é€‰æ‹©å¯¹æ¯”PDFæ–‡ä»¶ï¼ˆæœ€å¤š5ä¸ªï¼‰", 
                                    type=["pdf"], 
                                    accept_multiple_files=True)
    
    submitted = st.form_submit_button("å¼€å§‹åˆ†æ")

if submitted and base_file and compare_files:
    # é™åˆ¶æ–‡ä»¶æ•°é‡
    compare_files = compare_files[:max_files]
    
    if not qwen_api_key:
        st.warning("æœªæ£€æµ‹åˆ°APIå¯†é’¥ï¼Œåˆ†æåŠŸèƒ½å°†å—é™")
    
    try:
        # è¯»å–åŸºå‡†æ–‡ä»¶ï¼ˆè½¬æ¢ä¸ºå­—èŠ‚æµä»¥ä¾¿ç¼“å­˜ï¼‰
        base_bytes = base_file.getvalue()
        base_text = extract_text_from_pdf(base_bytes)
        
        if not base_text:
            st.error("æ— æ³•æå–åŸºå‡†æ–‡ä»¶æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶")
        else:
            # æ€»è¿›åº¦è·Ÿè¸ª
            total_files = len(compare_files)
            overall_progress = st.progress(0)
            
            for i, compare_file in enumerate(compare_files, 1):
                st.markdown(f'## ğŸ” æ¯”å¯¹åˆ†æ {i}/{total_files}: {base_file.name} vs {compare_file.name}')
                st.markdown('<div class="comparison-section">', unsafe_allow_html=True)
                
                # å¤„ç†å¯¹æ¯”æ–‡ä»¶
                compare_bytes = compare_file.getvalue()
                compare_text = extract_text_from_pdf(compare_bytes)
                
                if not compare_text:
                    st.error(f"æ— æ³•æå– {compare_file.name} çš„æ–‡æœ¬")
                else:
                    analyze_single_comparison(base_text, compare_text, 
                                            base_file.name, compare_file.name, 
                                            qwen_api_key)
                
                st.markdown('</div>', unsafe_allow_html=True)
                overall_progress.progress(i / total_files)
            
            overall_progress.empty()
            st.success("æ‰€æœ‰æ–‡ä»¶åˆ†æå®Œæˆï¼")
    
    except Exception as e:
        st.error(f"åº”ç”¨å‡ºé”™: {str(e)}")
        st.exception(e)  # æ˜¾ç¤ºè¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
else:
    if submitted:
        if not base_file:
            st.warning("è¯·ä¸Šä¼ åŸºå‡†PDFæ–‡ä»¶")
        if not compare_files:
            st.warning("è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªå¯¹æ¯”PDFæ–‡ä»¶")
    else:
        st.info('è¯·ä¸Šä¼ æ–‡ä»¶å¹¶ç‚¹å‡»"å¼€å§‹åˆ†æ"æŒ‰é’®')

# é¡µè„š
st.divider()
st.markdown("""
<div style="text-align: center; color: #666; margin-top: 2rem;">
    ä¸­æ–‡PDFæ¡æ¬¾åˆè§„æ€§åˆ†æå·¥å…· | ä¼˜åŒ–ç‰ˆ
</div>
""", unsafe_allow_html=True)
